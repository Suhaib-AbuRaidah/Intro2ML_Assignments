{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVHyEU7Bn5fWUhMgz5WGO7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Suhaib-AbuRaidah/Intro2ML_Assignments/blob/main/Assignment1/Assignment_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Five Ideas to use a smartphone's sensors to analyze data"
      ],
      "metadata": {
        "id": "95oX0vH3yw0L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**First idea:** Capture an image like a checkerboard for example, and detect the corners using either OpenCVâ€™s built-in corner detectors or a\n",
        "Harris corner detector implemented with NumPy.\n",
        "\n",
        "**Second idea:** Record a person speaking outdoors in a windy environment and\n",
        "apply a high-pass filter to reduce wind noise.\n",
        "\n",
        "**Third idea:** Capture multiple images of an object from different viewpoints\n",
        "and identify corresponding points in the images using ORB or SIFT feature\n",
        "extractors (non-AI based).\n",
        "\n",
        "**Fourth idea:** Record accelerometer data while moving, integrate and\n",
        "double-integrate the signals to estimate velocity and distance traveled,\n",
        "and then apply a PID controller or a Kalman filter for motion tracking.\n",
        "\n",
        "**Fifth idea:** Measure ambient light sensor (ALS) readings outdoors during\n",
        "daylight and compare them with reference clear-sky radiance data for the same\n",
        "time, date, and location to estimate cloudiness. (how cloudy the weather is)."
      ],
      "metadata": {
        "id": "xZMi-Q_UxDzf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Implementing the First Idea"
      ],
      "metadata": {
        "id": "pjZu1KFXyeHu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "xTe8FDtFmsz5"
      },
      "outputs": [],
      "source": [
        "# This cell is for importing necessary packages\n",
        "import cv2\n",
        "import numpy as np\n",
        "from google.colab.patches import cv2_imshow"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#This cell is for loading and displaying the image of the\n",
        "#checkerboard taking by my phone.\n",
        "\n",
        "img = cv2.imread('./Checkerboard.jpeg')\n",
        "cv2_imshow(img)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "id": "ay4cNj6k3EB2",
        "outputId": "0c835083-21f9-4080-d37f-ae4ae8a998c2"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "'NoneType' object has no attribute 'clip'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1318454111.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./Checkerboard.jpeg'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcv2_imshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/patches/__init__.py\u001b[0m in \u001b[0;36mcv2_imshow\u001b[0;34m(a)\u001b[0m\n\u001b[1;32m     16\u001b[0m       \u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mM\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0man\u001b[0m \u001b[0mNxM\u001b[0m \u001b[0mBGRA\u001b[0m \u001b[0mcolor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m   \"\"\"\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'uint8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0;31m# cv2 stores colors as BGR; convert to RGB\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'clip'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell is for cropping the background from the image and displaying it.\n",
        "\n",
        "img = img[45:550,250:920]\n",
        "cv2_imshow(img)"
      ],
      "metadata": {
        "id": "PNyqRZVxxt46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell is for defining the function that does the convolution over the\n",
        "# image, necssary for applying the differntiating filter.\n",
        "\n",
        "def convolve(image, kernel):\n",
        "    H, W = image.shape\n",
        "    kH, kW = kernel.shape\n",
        "    offsetH = kH // 2 # equals 1 for a kernel of shape (3,3), it means that the first and last line will be skipped in the x-dimension.\n",
        "    offsetW = kW // 2 # The same here, but for the y-dimension.\n",
        "\n",
        "    output = np.zeros_like(image)\n",
        "    # looping in the pixels to convolve with the Sobel kernel.\n",
        "    for i in range(offsetH, H - offsetH):\n",
        "        for j in range(offsetW, W - offsetW):\n",
        "            region = image[i - offsetH:i + offsetH + 1, j - offsetW:j + offsetW + 1] # choosing part of the image (3,3) to covolve.\n",
        "            output[i, j] = np.sum(region * kernel) # pixel-wise multiplication with the kernel and summing the result for each pixel.\n",
        "\n",
        "    return output"
      ],
      "metadata": {
        "id": "JAsCRwLp4blP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#This cell is for defining the Harris corner detector algorithm function,\n",
        "# this algorithm is done from scratch using only numpy.\n",
        "\n",
        "def harris_corner_detection(image, k=0.04, window_size=3):\n",
        "    # Load and convert the image to grayscale\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    gray = np.float32(gray)\n",
        "\n",
        "    # Define Sobel kernels in x and y dimensions to find the gradient.\n",
        "    sobel_x = np.array([[-1, 0, 1],\n",
        "                    [-2, 0, 2],\n",
        "                    [-1, 0, 1]], dtype=np.float64)\n",
        "\n",
        "    sobel_y = np.array([[-1, -2, -1],\n",
        "                    [ 0,  0,  0],\n",
        "                    [ 1,  2,  1]], dtype=np.float64)\n",
        "\n",
        "    # Compute image gradients using convolve function with Sobel kernels\n",
        "    Ix = convolve(gray, sobel_x)\n",
        "    Iy = convolve(gray, sobel_y)\n",
        "\n",
        "    # Compute products of derivatives\n",
        "    Ixx = Ix * Ix\n",
        "    Iyy = Iy * Iy\n",
        "    Ixy = Ix * Iy\n",
        "\n",
        "    # Define window size\n",
        "    kernel = np.ones((window_size, window_size))\n",
        "\n",
        "    # Convolve with window of ones.\n",
        "    Sxx = convolve(Ixx, kernel)\n",
        "    Syy = convolve(Iyy, kernel)\n",
        "    Sxy = convolve(Ixy, kernel)\n",
        "\n",
        "    # Compute Harris response\n",
        "    detM = (Sxx * Syy)-(Sxy ** 2)\n",
        "    traceM = Sxx + Syy\n",
        "    R = detM - k * ((traceM/2) ** 2) # R is the Harris response of the image\n",
        "    image[R>0.01*R.max()]= [0,0,255] # Find the corners by comparing Harris response to a threshold of [0.01*R.max()]\n",
        "    # Show the image\n",
        "    cv2_imshow(image)\n",
        "\n",
        "    return image"
      ],
      "metadata": {
        "id": "W1oIC3Mw4s7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell is for defining the function to use the built-in Open-CV Harris\n",
        "# corner detector function. It is used as a baseline for the algorithm in the\n",
        "# cell above.\n",
        "\n",
        "def built_in_harris(image):\n",
        "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    gray = np.float32(gray)\n",
        "    dst = cv2.cornerHarris(gray, 2, 3, 0.1)\n",
        "    dst = cv2.dilate(dst, None)\n",
        "    image[dst > 0.01*dst.max()] = [0, 0, 255]\n",
        "    cv2_imshow(image)"
      ],
      "metadata": {
        "id": "dqwRvz_y4w4U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell is for applying the built-in Harris corner detector in the\n",
        "# Open-CV library on the image. (The red dots are clearly the corners)\n",
        "\n",
        "img_built_in = np.copy(img)\n",
        "built_in_harris(img_built_in)"
      ],
      "metadata": {
        "id": "bDxOEI6z9Rq_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This cell is for applying the Harris detector algorithm implemented using\n",
        "# numpy on the image.\n",
        "\n",
        "img_manual = np.copy(img)\n",
        "corners = harris_corner_detection(img_manual, k=0.4, window_size=5) # takes half a minute"
      ],
      "metadata": {
        "id": "LnE7A8SI42Hs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analysis:\n",
        "It can be seen that the performance of the algorithm of the Harris corner detector is very good, it detects all the corners of the checkerboard whether internal corners or corners at the boundaries.\n",
        "\n",
        "Compared to the baseline, it's as consistant as the built-in Harris detector function from the Open-CV library where the results of the two look identical.\n",
        "\n",
        "It can be noticed that the interior conrners of the checkerboard (dots in red) are thicker than the corners at the boundaries, this is the case because on a checkerboard, interior corners have two strong edges in both x and y directions, so the response spreads out more. In addition that Harris corner detector computes a score (the Harris response) at every pixel based on gradient changes in a window, where two strong, perpendicular edges meet, the response is high across a small neighborhood, not just one pixel.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JezbmAs9UAJG"
      }
    }
  ]
}